---
layout: archive
title: "Highlight"
permalink: /highlight/
author_profile: true
---
### Research highlights

- **GraspGPT (IEEE RA-L 2023)**: Leveraging semantic knowledge from a large language model for task-oriented grasping.  
  [Paper](https://arxiv.org/pdf/2307.13204)

  <details>
    <summary><strong>Video (watch here)</strong></summary>
    <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;margin-top:10px;">
      <iframe src="https://www.youtube.com/embed/qq0DMdHRw1E" title="GraspGPT video" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </div>
  </details>

- **FoundationGrasp (IEEE T-ASE 2025)**: Generalizable task-oriented grasping with foundation models.  
  [Paper](https://arxiv.org/pdf/2404.10399)

  <details>
    <summary><strong>Video (watch here)</strong></summary>
    <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;margin-top:10px;">
      <iframe src="https://www.youtube.com/embed/B6iTa6BRB1w" title="FoundationGrasp video" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </div>
  </details>

- **HGDiffuser (IROS 2025)**: Efficient task-oriented grasp generation via human-guided grasp diffusion models.  
  [Paper](https://arxiv.org/pdf/2503.00508) Â· [Code](https://github.com/red0orange/handgrasp_ws)

  <details>
    <summary><strong>Video (watch here)</strong></summary>
    <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;margin-top:10px;">
      <iframe src="https://www.youtube.com/embed/fUt6cE9SZoY" title="HGDiffuser video" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </div>
  </details>

- **Multi-view Fusion for Object Rearrangement (ICRA 2024)**: Efficient object rearrangement via multi-view fusion.  
  [Paper](https://arxiv.org/pdf/2309.08994)

  <details>
    <summary><strong>Video (watch here)</strong></summary>
    <div style="position:relative;padding-bottom:56.25%;height:0;overflow:hidden;margin-top:10px;">
      <iframe src="https://www.youtube.com/embed/oUlDwmMbjQU" title="Multi-view rearrangement video" loading="lazy" style="position:absolute;top:0;left:0;width:100%;height:100%;" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
    </div>
  </details>

- **CloudEdgeSLAM (IEEE TVT 2023)**: Cloud learning meets edge model-based SLAM (cloud-edge collaborative system).  
  [Paper](https://ieeexplore.ieee.org/abstract/document/10264105)

### Competitions

- **RoboMaster Robotics Competition** (Algorithm Group Leader): National Second Prize (2020)
- **National College Students' Smart Car Competition** (Team Leader): National Third Prize (2020)
- **American Interdisciplinary Contest in Modeling (ICM/MCM)**: Meritorious Winner (top 7.09%) (2021)

### Awards and honors

- **The 1st Academic Research Star of Zhongguancun Academy** (2026)
- **SCNU First-Class Scholarship** (2020)

